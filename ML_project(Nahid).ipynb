{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPaXA6iUEeR6sbnUFj/q/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/28nahidhasan/ML_final-project/blob/main/ML_project(Nahid).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "7sGFHhPqRu7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle scikit-learn tensorflow imbalanced-learn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "WuMODgH9RxSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and merge datasets"
      ],
      "metadata": {
        "id": "KGQmSk2_SEao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "file_list = [f for f in uploaded.keys() if 'Dhaka_PM2.5' in f]\n",
        "dfs = [pd.read_csv(f) for f in file_list]\n",
        "merged_df = pd.concat(dfs, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "JxzMk6PmSKG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df[merged_df['AQI'] != -999]\n",
        "merged_df['AQI Category'] = merged_df['AQI Category'].fillna('Unknown')"
      ],
      "metadata": {
        "id": "wIlkE1yhS908"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering\n",
        "merged_df['Date (LT)'] = pd.to_datetime(merged_df['Date (LT)'], format='%d/%m/%Y %H:%M')\n",
        "merged_df['Hour'] = merged_df['Date (LT)'].dt.hour\n",
        "merged_df['Month'] = merged_df['Date (LT)'].dt.month\n",
        "merged_df['DayOfWeek'] = merged_df['Date (LT)'].dt.dayofweek\n",
        "\n",
        "# Create classification target\n",
        "bins = [0, 50, 100, 150, 200, 300, np.inf]\n",
        "labels = ['Good','Moderate','USG','Unhealthy','VUnhealthy','Hazardous']\n",
        "merged_df['AQI_Class'] = pd.cut(merged_df['AQI'], bins=bins, labels=labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "JBx38Z-hTKC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "merged_df['AQI_Class_Encoded'] = le.fit_transform(merged_df['AQI_Class'])\n",
        "\n",
        "# Save processed data\n",
        "processed_filename = 'processed_air_quality.csv'\n",
        "merged_df.to_csv(processed_filename, index=False)\n",
        "files.download(processed_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sn0IFMfJTY7b",
        "outputId": "da6281b3-a944-4565-f7cc-581e62440ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ecb882c8-7e99-4626-9f88-58cc052759d3\", \"processed_air_quality.csv\", 3918601)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA SPLITTING"
      ],
      "metadata": {
        "id": "3YizjqShTjs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['NowCast Conc.', 'Raw Conc.', 'Hour', 'Month', 'DayOfWeek']\n",
        "target = 'AQI_Class_Encoded'\n",
        "\n",
        "X = merged_df[features]\n",
        "y = merged_df[target]\n",
        "\n",
        "# Split data (70% train, 15% validation, 15% test)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    X, y, test_size=0.15, stratify=y, random_state=42\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=42  # 0.15/0.85 ≈ 0.1765\n",
        ")\n",
        "\n",
        "# Handle class imbalance\n",
        "smote = SMOTE(random_state=42, k_neighbors=1)\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "YuFhsk_yTqDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FOCAL LOSS IMPLEMENTATION"
      ],
      "metadata": {
        "id": "S6CfORTOUDbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def focal_loss(gamma=2., alpha=4.):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        epsilon = tf.keras.backend.epsilon()\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        loss = alpha * tf.pow(1. - y_pred, gamma) * cross_entropy\n",
        "        return tf.reduce_mean(loss, axis=-1)\n",
        "    return focal_loss_fixed\n"
      ],
      "metadata": {
        "id": "UZJVlbAZUIGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "ml44iG_rUNnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network"
      ],
      "metadata": {
        "id": "rQTDn1YKUdcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NEURAL NETWORK MODEL\n",
        "def create_nn_model(input_shape, n_classes):\n",
        "    model = Sequential([\n",
        "        Dense(256, activation='relu', input_shape=(input_shape,)),\n",
        "        Dropout(0.4),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(n_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss=focal_loss(),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Preprocess data for NN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train NN\n",
        "nn_model = create_nn_model(X_train_scaled.shape[1], len(le.classes_))\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "history = nn_model.fit(\n",
        "    X_train_scaled, tf.keras.utils.to_categorical(y_train),\n",
        "    validation_data=(X_val_scaled, tf.keras.utils.to_categorical(y_val)),\n",
        "    epochs=200,\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "aP_AgRp_UQyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRADITIONAL MODELS"
      ],
      "metadata": {
        "id": "QTyoiqIGUkqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced')\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [200, 300],\n",
        "        'max_depth': [None, 20],\n",
        "        'min_samples_split': [2, 3]\n",
        "    },\n",
        "    'KNN': {\n",
        "        'n_neighbors': [3, 5],\n",
        "        'weights': ['distance']\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'max_depth': [None],\n",
        "        'min_samples_split': [2]\n",
        "    }\n",
        "}\n",
        "\n",
        "best_models = {}\n",
        "for name in models:\n",
        "    grid = GridSearchCV(\n",
        "        models[name],\n",
        "        param_grids[name],\n",
        "        cv=5,\n",
        "        scoring='accuracy',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    grid.fit(X_train, y_train)\n",
        "    best_models[name] = grid.best_estimator_\n",
        "    print(f\"{name} Best Params: {grid.best_params_}\")\n",
        "    print(f\"{name} Best CV Accuracy: {grid.best_score_:.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "O1ZAW8dkUpph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION & VISUALIZATION"
      ],
      "metadata": {
        "id": "4VQKwfXqWWQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training History Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Neural Network Training History')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_3HUWAofWZtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Comparison\n",
        "results = {}\n",
        "for name, model in best_models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    val_pred = model.predict(X_val)\n",
        "    test_pred = model.predict(X_test)\n",
        "\n",
        "    results[name] = {\n",
        "        'Train Accuracy': accuracy_score(y_train, model.predict(X_train)),\n",
        "        'Validation Accuracy': accuracy_score(y_val, val_pred),\n",
        "        'Test Accuracy': accuracy_score(y_test, test_pred)\n",
        "    }"
      ],
      "metadata": {
        "id": "66opYg3dX_P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Evaluation\n",
        "nn_val_pred = np.argmax(nn_model.predict(X_val_scaled), axis=1)\n",
        "nn_test_pred = np.argmax(nn_model.predict(X_test_scaled), axis=1)\n",
        "results['Neural Network'] = {\n",
        "    'Train Accuracy': accuracy_score(y_train, np.argmax(nn_model.predict(X_train_scaled), axis=1)),\n",
        "    'Validation Accuracy': accuracy_score(y_val, nn_val_pred),\n",
        "    'Test Accuracy': accuracy_score(y_test, nn_test_pred)\n",
        "}"
      ],
      "metadata": {
        "id": "l4MdB8BeYCdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results DataFrame"
      ],
      "metadata": {
        "id": "e5mbWbF_YWbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Results DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
        "\n",
        "# Accuracy Comparison Plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "results_df[['Train Accuracy', 'Validation Accuracy', 'Test Accuracy']].plot(kind='bar')\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.95, 1.005)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "BydNB_XRYY50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix for Best Model"
      ],
      "metadata": {
        "id": "G32wMMEXYki2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for Best Model\n",
        "best_model_name = results_df.index[0]\n",
        "if best_model_name == 'Neural Network':\n",
        "    y_pred = nn_test_pred\n",
        "else:\n",
        "    y_pred = best_models[best_model_name].predict(X_test)\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred),\n",
        "            annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.title(f'Best Model Confusion Matrix ({best_model_name})')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WxUpkF8rYsXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Importance"
      ],
      "metadata": {
        "id": "K44KvreMY3tS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance (For Tree-based Models)\n",
        "if 'Random Forest' in best_models:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    importances = best_models['Random Forest'].feature_importances_\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    plt.title(\"Feature Importances (Random Forest)\")\n",
        "    plt.barh(range(len(indices)), importances[indices], align='center')\n",
        "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1Ffa8VrZY493"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AQI Distribution Plot"
      ],
      "metadata": {
        "id": "5YodK7xbZBhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AQI Distribution Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "merged_df['AQI_Class'].value_counts().plot(kind='bar', color='teal')\n",
        "plt.title('AQI Class Distribution')\n",
        "plt.xlabel('Air Quality Category')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2N6Z0HBgY_z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FUTURE PREDICTION"
      ],
      "metadata": {
        "id": "UZRlF4ABabKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "main"
      ],
      "metadata": {
        "id": "Fy-N7VEmtU1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First ensure these imports are present\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "#  AQI CALCULATION & HEALTH ADVISORY\n",
        "def calculate_aqi_class(aqi_value):\n",
        "    \"\"\"Official EPA AQI classification\"\"\"\n",
        "    if aqi_value <= 50:\n",
        "        return 'Good'\n",
        "    elif aqi_value <= 100:\n",
        "        return 'Moderate'\n",
        "    elif aqi_value <= 150:\n",
        "        return 'Unhealthy for Sensitive Groups'\n",
        "    elif aqi_value <= 200:\n",
        "        return 'Unhealthy'\n",
        "    elif aqi_value <= 300:\n",
        "        return 'Very Unhealthy'\n",
        "    else:\n",
        "        return 'Hazardous'\n",
        "\n",
        "def health_status(aqi_class, aqi_value):\n",
        "    \"\"\"Enhanced health advisory with actionable advice\"\"\"\n",
        "    advisories = {\n",
        "        'Good': {\n",
        "            'action': \"Enjoy outdoor activities\",\n",
        "            'advice': [\"No restrictions needed\"]\n",
        "        },\n",
        "        'Moderate': {\n",
        "            'action': \"Sensitive people should reduce prolonged exertion\",\n",
        "            'advice': [\"Consider outdoor activity timing\", \"Monitor air quality changes\"]\n",
        "        },\n",
        "        'Unhealthy for Sensitive Groups': {\n",
        "            'action': \"Children & elderly should limit outdoor activity\",\n",
        "            'advice': [\"Close windows during peak hours\", \"Use air purifiers\"]\n",
        "        },\n",
        "        'Unhealthy': {\n",
        "            'action': \"Everyone should limit outdoor exertion\",\n",
        "            'advice': [\"Avoid strenuous activities\", \"Wear N95 masks outdoors\"]\n",
        "        },\n",
        "        'Very Unhealthy': {\n",
        "            'action': \"Avoid all outdoor activities\",\n",
        "            'advice': [\"Seal windows and doors\", \"Run air purifiers continuously\"]\n",
        "        },\n",
        "        'Hazardous': {\n",
        "            'action': \"Remain indoors with filtered air\",\n",
        "            'advice': [\"Seek alternative shelter if needed\", \"Follow emergency advisories\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    status = advisories.get(aqi_class, {'action': 'Unknown', 'advice': []})\n",
        "    return (\n",
        "        f\"AQI: {aqi_value:.0f} ({aqi_class})\\n\"\n",
        "        f\"Immediate Action: {status['action']}\\n\"\n",
        "        \"Recommended Steps:\\n- \" + '\\n- '.join(status['advice'])\n",
        "    )\n",
        "\n",
        "# ENHANCED PREDICTION INTERFACE\n",
        "def get_date_prediction():\n",
        "    print(\"\\n=== Date-Based AQI Forecast ===\")\n",
        "    try:\n",
        "        # Improved date parsing\n",
        "        date_str = input(\"Enter prediction date (DD/MM/YYYY HH:MM): \")\n",
        "        dt = pd.to_datetime(date_str, dayfirst=True, format='mixed')\n",
        "\n",
        "        # Validate date range\n",
        "        if dt < pd.to_datetime('2016-01-01') or dt > pd.to_datetime('2025-12-31'):\n",
        "            raise ValueError(\"Date out of model range (2016-2025)\")\n",
        "\n",
        "        # Get historical patterns (ensure historical_df exists)\n",
        "        # You need to define historical_df and merged_df with proper data\n",
        "        mask = (\n",
        "            (historical_df['Month'] == dt.month) &\n",
        "            (historical_df['DayOfWeek'] == dt.weekday()) &\n",
        "            (historical_df['Hour'] == dt.hour)\n",
        "        )\n",
        "\n",
        "        if mask.any():\n",
        "            features = historical_df[mask].iloc[0][['NowCast Conc.', 'Raw Conc.']]\n",
        "        else:\n",
        "            features = pd.Series({\n",
        "                'NowCast Conc.': merged_df['NowCast Conc.'].median(),\n",
        "                'Raw Conc.': merged_df['Raw Conc.'].median()\n",
        "            })\n",
        "\n",
        "        # Official AQI calculation (using PM2.5 conversion)\n",
        "        pm25 = features['NowCast Conc.']\n",
        "        aqi_value = official_aqi_calculation(pm25)\n",
        "        aqi_class = calculate_aqi_class(aqi_value)\n",
        "\n",
        "        # Display results\n",
        "        print(f\"\\n=== Prediction for {dt.strftime('%a %d %b %Y %H:%M')} ===\")\n",
        "        print(f\"Estimated PM2.5: {pm25:.1f} µg/m³\")\n",
        "        print(health_status(aqi_class, aqi_value))\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n⚠️ Error: {str(e)}\")\n",
        "\n",
        "#  OFFICIAL AQI CALCULATION\n",
        "def official_aqi_calculation(pm25):\n",
        "    \"\"\"EPA's official PM2.5 to AQI conversion\"\"\"\n",
        "    breakpoints = [\n",
        "        (0.0, 12.0, 0, 50),\n",
        "        (12.1, 35.4, 51, 100),\n",
        "        (35.5, 55.4, 101, 150),\n",
        "        (55.5, 150.4, 151, 200),\n",
        "        (150.5, 250.4, 201, 300),\n",
        "        (250.5, 500.4, 301, 500)\n",
        "    ]\n",
        "\n",
        "    for bp in breakpoints:\n",
        "        if bp[0] <= pm25 <= bp[1]:\n",
        "            return ((bp[3] - bp[2])/(bp[1] - bp[0])) * (pm25 - bp[0]) + bp[2]\n",
        "    return 500\n",
        "\n",
        "# Example usage\n",
        "get_date_prediction()"
      ],
      "metadata": {
        "id": "DWDpOYHExTwn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}